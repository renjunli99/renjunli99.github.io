<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" 
        content="Learning Humanoid Locomotion with Perceptive Internal Model">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <!-- <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/overview.pdf" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->

<!-- 
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Reinforcement Learning, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Learning Humanoid Locomotion with Perceptive Internal Model</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- <link rel="icon" type="image/x-icon" href="static/images/icon.png"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Humanoid Locomotion with Perceptive Internal Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://junfeng-long.github.io" target="_blank">Junfeng Long</a><sup>*,1</sup>,</span>
              <span class="author-block">
                <a href="https://ieeexplore.ieee.org/author/37088376921" target="_blank">Junli Ren</a><sup>*,1,2</sup>,</span>
              <span class="author-block">
                <a href="https://smoggy-p.github.io/" target="_blank">Moji Shi</a><sup>*,1,</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=Vc3DCUIAAAAJ" target="_blank">Zirui Wang</a><sup>1,3</sup>,</span>
              <span class="author-block">
                <a href="https://taohuang13.github.io" target="_blank">Tao Huang</a><sup>1,4</sup>,</span>
              <span class="author-block">
                <a href="http://luoping.me" target="_blank">Ping Luo</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://oceanpang.github.io" target="_blank">Jiangmiao Pang</a><sup>†,1</sup>,</span>
              <span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Shanghai AI Laboratory,</span>
                    <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>
                    <span class="author-block"><sup>3</sup>Zhejiang University,</span>
                    <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- PDF link -->
                      <span class="link-block">
                        <a href="./static/pdfs/PIM.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/OpenRobotLab/HIMLoco" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Comming Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.14386" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/Sgp3Yr-EILs?si=DMEnsNPFdi2hI5Vl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </center>
    </div>
      <h2 class="subtitle has-text-centered">
        Our locomotion policy can drive humanoid robots to walk across difficult terrains. It is powered by Perceptive Internal Model that uses the robot’s historical internal states to simulate
        implicit response and estimate the robot's velocity with the integrating of perceptive information so that the policy can estimate disturbances from environmental dynamics.
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper presents a Hybrid Internal Model (HIM) based method for legged locomotion control in quadruped robots. The method aims to address the limitations of existing learning-based locomotion control paradigms, which suffer from information losses, noisy observations, sample efficiency, and difficulties in developing general locomotion policies for robots with different sensor configurations. The proposed HIM method leverages joint encoders and an Inertial Measurement Unit (IMU) as the only sensors for predicting robot states. Our framework consists of two components: the information extractor HIM and the policy network. Unlike previous methods that explicitly model environmental observations such as ground elevation, friction, restitution, etc, HIM only explicitly estimates velocity and encodes other environmental dynamics-related properties as an implicit latent embedding, we call this dynamics-aware latent since it represents the dynamics information of the robot system in certain environments. The dynamics-aware latent is learned through contrastive learning, which enhances robustness and adaptability in disturbed and unpredictable environments. The proposed method is validated through simulations in different terrains and real-world experiments on the Unitree robots. The results demonstrate that HIM achieves substantial agility over challenging terrains with minimal sensors and fast convergence.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          In contrast to quadruped robots that can navigate diverse terrains using a blind policy, humanoid robots require accurate perception for 
          stable locomotion due to their high degrees of freedom and inherently unstable morphology. However, incorporating perceptual signals often 
          introduces additional disturbances to the system, potentially reducing its robustness, generalizability, and efficiency. This paper presents 
          the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its 
          surroundings. We train the policy using ground-truth obstacle heights surrounding the robot in simulation, optimizing it based on the Hybrid Internal Model (HIM), 
          and perform inference with heights sampled from the constructed elevation map. Unlike previous methods that directly encode depth maps or raw point clouds, 
          our approach allows the robot to perceive the terrain beneath its feet clearly and is less affected by camera movement or noise. Furthermore, since depth map 
          rendering is not required in simulation, our method introduces minimal additional computational costs and can train the policy in 3 hours on an RTX 4090 GPU. 
          We verify the effectiveness of our method across various humanoid robots, various indoor and outdoor terrains, stairs, and various sensor configurations. 
          Our method can enable a humanoid robot to continuously climb stairs and has the potential to serve as a foundational algorithm for the development of 
          future humanoid control methods.
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <h3 class="title is-4">Framework Overview</h3>
    </div>
    <div class="content has-text-centered">
      <img width="75%" src="./static/images/method.png">
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          Within PIM, we integrate perceptive information into the state predictor to achieve more comprehensive and accurate state prediction. 
          A LiDAR-based elevation map serves as the perception model, enabling more precise perception alignment between simulation and real-world environments.
        </div>
      </div>
    </div>

      <!-- <div class="content has-text-centered">
        <video id="replay-video"
               controls
               muted
               preload
               playsinline
               width="75%">
          <source src="./static/videos/demo.mov"
                  type="video/mov">
        <p>
          The overview of HIMLoco.
        </p>
        </video>
      </div> -->
      <!--/ Pipeline -->
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h3 class="title is-4"> The robot walks throught human-level stairs. </h3>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
        <div class="column">
          <div class="content">
            <center>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/8wsh-2jXhHQ?si=vwh9lxp7UD2KuhWZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </center>
          </div>
        </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h3 class="title is-4"> The robot steps on a 50cm platform and then steps over a 70cm gap. </h3>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <center>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/YS4ibyy-yAk?si=32Jrf87asLV9tqN2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </center>
        </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h3 class="title is-4"> The robot walks throught different outdoor terrains.</h3>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <center>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/If6_a89RQvg?si=T8lOADMlVLgimjeU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </center>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h3 class="title is-4"> Fourier GR1 walks throught human-level soft stairs.</h3>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <center>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1LVGIGyyza0?si=0Wmdy7n-2mO0spy-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </center>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h3 class="title is-4"> Fourier GR1 outdoor walk.</h3>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <center>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1qSzlIp_aQk?si=tkkmIxDQNDiksqOF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </center>
        </div>
      </div>
  </div>
</section>



<!-- End image carousel -->




<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!--Acknowledgement -->
<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Acknowledgement</h2>
    <div class="content has-text-justified">
      This work was supported by Shanghai AI Laboratory. We want to thank Rongzhang Gu, Jia Zeng, and Xuekun Jiang for their help in the creating of demo video. We also want 
      to thank the colleagues from Fourier Intelligence for their help with the experiments of the GR1 robot.
    </div>
  </div>
<!--End BibTex citation -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{long2024learninghumanoidlocomotionperceptive,
          title={Learning Humanoid Locomotion with Perceptive Internal Model}, 
          author={Junfeng Long and Junli Ren and Moji Shi and Zirui Wang and Tao Huang and Ping Luo and Jiangmiao Pang},
          year={2024},
          eprint={2411.14386},
          archivePrefix={arXiv},
          primaryClass={cs.RO},
          url={https://arxiv.org/abs/2411.14386}, 
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. <a href="https://mapmyvisitors.com/web/1bx9w"><img src="https://mapmyvisitors.com/map.png?d=FJYHrRx0PysSwKEtLGLCTegrk1WsH9UFQvELHvvYqcg&cl=ffffff" style="width:0%"/></a>
            <a href="https://clustrmaps.com/site/1bz93"><img src="//www.clustrmaps.com/map_v2.png?d=jetxJV6zCGqIrAyCLI2LeFBrPty6tUkrHEsNrcd2JCY&cl=ffffff" style="width:0%"/></a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
